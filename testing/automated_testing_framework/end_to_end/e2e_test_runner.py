#!/usr/bin/env python3
"""
PowerAutomation ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨

ç»Ÿä¸€ç®¡ç†å’Œæ‰§è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•
"""

import os
import sys
import yaml
import pytest
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class E2ETestRunner:
    """ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.e2e_dir = Path(__file__).parent
        self.config_path = self.e2e_dir / "configs" / "e2e_config.yaml"
        self.config = self._load_config()
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        self.report_dir = self.e2e_dir / "e2e_reports"
        self.report_dir.mkdir(exist_ok=True)
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        if self.config_path.exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}
    
    def run_all_e2e_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ‰§è¡ŒPowerAutomationç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶...")
        
        test_config = self.config.get("end_to_end_tests", {})
        execution_order = test_config.get("execution_order", [])
        
        results = {}
        overall_success = True
        
        for test_level in execution_order:
            print(f"\nğŸ“‹ æ‰§è¡Œ {test_level} æµ‹è¯•...")
            
            result = self._run_test_level(test_level)
            results[test_level] = result
            
            if not result["success"]:
                overall_success = False
                print(f"âŒ {test_level} æµ‹è¯•å¤±è´¥")
            else:
                print(f"âœ… {test_level} æµ‹è¯•æˆåŠŸ")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report(results)
        
        if overall_success:
            print("\nğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print("\nâš ï¸ éƒ¨åˆ†ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
        
        return overall_success
    
    def _run_test_level(self, test_level: str) -> Dict[str, Any]:
        """è¿è¡Œç‰¹å®šçº§åˆ«çš„æµ‹è¯•"""
        test_dir = self.e2e_dir / test_level
        
        if not test_dir.exists():
            return {
                "success": False,
                "error": f"æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}",
                "execution_time": 0
            }
        
        # æ„å»ºpytestå‚æ•°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.report_dir / f"{test_level}_report_{timestamp}.html"
        
        pytest_args = [
            str(test_dir),
            "-v",
            "--tb=short",
            "--capture=no",
            f"--html={report_file}",
            "--self-contained-html"
        ]
        
        try:
            start_time = datetime.now()
            result = pytest.main(pytest_args)
            end_time = datetime.now()
            
            execution_time = (end_time - start_time).total_seconds()
            
            return {
                "success": result == 0,
                "exit_code": result,
                "execution_time": execution_time,
                "report_file": str(report_file),
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "execution_time": 0
            }
    
    def run_specific_test(self, test_level: str, test_name: str = None) -> bool:
        """è¿è¡Œç‰¹å®šçš„æµ‹è¯•"""
        test_dir = self.e2e_dir / test_level
        
        if test_name:
            test_file = test_dir / f"test_{test_name}.py"
            if not test_file.exists():
                print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
                return False
            target = str(test_file)
        else:
            target = str(test_dir)
        
        pytest_args = [target, "-v"]
        result = pytest.main(pytest_args)
        
        return result == 0
    
    def _generate_comprehensive_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_data = {
            "execution_timestamp": timestamp,
            "overall_success": all(r.get("success", False) for r in results.values()),
            "test_results": results,
            "summary": {
                "total_test_levels": len(results),
                "successful_levels": sum(1 for r in results.values() if r.get("success", False)),
                "failed_levels": sum(1 for r in results.values() if not r.get("success", False)),
                "total_execution_time": sum(r.get("execution_time", 0) for r in results.values())
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_path = self.report_dir / f"e2e_comprehensive_report_{timestamp}.json"
        import json
        with open(json_report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {json_report_path}")

if __name__ == "__main__":
    runner = E2ETestRunner()
    
    if len(sys.argv) > 1:
        # è¿è¡Œç‰¹å®šæµ‹è¯•
        test_level = sys.argv[1]
        test_name = sys.argv[2] if len(sys.argv) > 2 else None
        runner.run_specific_test(test_level, test_name)
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        runner.run_all_e2e_tests()
