#!/usr/bin/env python3
"""
PowerAutomation ç«¯åˆ°ç«¯æµ‹è¯•å±‚çº§ç®¡ç†å™¨

ç®¡ç†ç«¯åˆ°ç«¯æµ‹è¯•çš„å±‚çº§ç»“æ„ï¼Œé›†æˆå…œåº•è‡ªåŠ¨åŒ–æµ‹è¯•
æ”¯æŒå‰ç½®æ¡ä»¶ç³»ç»Ÿå’Œå¹³å°é€‰æ‹©æœºåˆ¶
"""

import os
import sys
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

class EndToEndTestManager:
    """ç«¯åˆ°ç«¯æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.e2e_dir = Path(__file__).parent
        self.test_root = self.e2e_dir.parent
        
        # ç«¯åˆ°ç«¯æµ‹è¯•å­æ¨¡å—
        self.modules = {
            "client_side": self.e2e_dir / "client_side",
            "server_side": self.e2e_dir / "server_side", 
            "integration": self.e2e_dir / "integration",
            "fallback_automation": self.e2e_dir / "fallback_automation"
        }
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡å—ç›®å½•å­˜åœ¨
        for module_dir in self.modules.values():
            module_dir.mkdir(exist_ok=True)
            (module_dir / "__init__.py").touch()
    
    def initialize_e2e_structure(self) -> bool:
        """åˆå§‹åŒ–ç«¯åˆ°ç«¯æµ‹è¯•ç»“æ„"""
        print("ğŸ—ï¸ åˆå§‹åŒ–ç«¯åˆ°ç«¯æµ‹è¯•ç»“æ„...")
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯æµ‹è¯•
            self._create_client_side_tests()
            
            # åˆ›å»ºæœåŠ¡ç«¯æµ‹è¯•
            self._create_server_side_tests()
            
            # åˆ›å»ºé›†æˆæµ‹è¯•
            self._create_integration_tests()
            
            # åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•é…ç½®
            self._create_e2e_config()
            
            # åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨
            self._create_e2e_runner()
            
            print("âœ… ç«¯åˆ°ç«¯æµ‹è¯•ç»“æ„åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•ç»“æ„åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _create_client_side_tests(self):
        """åˆ›å»ºå®¢æˆ·ç«¯æµ‹è¯•"""
        client_test_content = '''#!/usr/bin/env python3
"""
PowerAutomation å®¢æˆ·ç«¯ç«¯åˆ°ç«¯æµ‹è¯•

æµ‹è¯•å®¢æˆ·ç«¯åŠŸèƒ½çš„ç«¯åˆ°ç«¯æµç¨‹
"""

import pytest
import sys
from pathlib import Path

# æ·»åŠ æµ‹è¯•æ¡†æ¶è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))
from test_preconditions import PreconditionValidator

class TestClientSideE2E:
    """å®¢æˆ·ç«¯ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.validator = PreconditionValidator()
        cls.test_config = {
            "test_id": "CLIENT_E2E_001",
            "test_name": "å®¢æˆ·ç«¯ç«¯åˆ°ç«¯æµ‹è¯•",
            "preconditions": {
                "platform": {
                    "required_platforms": ["windows", "macos"],
                    "preferred_platforms": ["windows"],
                    "excluded_platforms": []
                },
                "resources": {
                    "min_memory_gb": 8,
                    "min_cpu_cores": 4,
                    "gpu_required": False
                },
                "capabilities": ["ui_test", "automation_test"],
                "environment": {
                    "os_version": "Windows 10+ / macOS 12.0+",
                    "automation_framework": "PowerAutomation 2.0+"
                },
                "dependencies": ["automation_engine", "ui_framework"]
            }
        }
    
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®"""
        validation_result = self.validator.validate_preconditions(
            self.test_config["preconditions"]
        )
        
        if not validation_result["valid"]:
            pytest.skip(f"å‰ç½®æ¡ä»¶ä¸æ»¡è¶³: {validation_result['reason']}")
    
    def test_client_startup_flow(self):
        """æµ‹è¯•å®¢æˆ·ç«¯å¯åŠ¨æµç¨‹"""
        # æ¨¡æ‹Ÿå®¢æˆ·ç«¯å¯åŠ¨
        startup_result = self._simulate_client_startup()
        
        assert startup_result["success"], f"å®¢æˆ·ç«¯å¯åŠ¨å¤±è´¥: {startup_result['error']}"
        assert startup_result["ui_loaded"], "UIç•Œé¢åŠ è½½å¤±è´¥"
        assert startup_result["services_ready"], "æœåŠ¡æœªå°±ç»ª"
    
    def test_client_automation_workflow(self):
        """æµ‹è¯•å®¢æˆ·ç«¯è‡ªåŠ¨åŒ–å·¥ä½œæµ"""
        # æ‰§è¡Œè‡ªåŠ¨åŒ–å·¥ä½œæµ
        workflow_result = self._execute_automation_workflow()
        
        assert workflow_result["workflow_completed"], "è‡ªåŠ¨åŒ–å·¥ä½œæµæœªå®Œæˆ"
        assert workflow_result["tasks_executed"] > 0, "æ²¡æœ‰æ‰§è¡Œä»»ä½•ä»»åŠ¡"
    
    def test_client_error_handling(self):
        """æµ‹è¯•å®¢æˆ·ç«¯é”™è¯¯å¤„ç†"""
        # æ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ
        error_result = self._simulate_client_error()
        
        assert error_result["error_handled"], "é”™è¯¯æœªè¢«æ­£ç¡®å¤„ç†"
        assert error_result["recovery_successful"], "é”™è¯¯æ¢å¤å¤±è´¥"
    
    def _simulate_client_startup(self) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå®¢æˆ·ç«¯å¯åŠ¨"""
        return {
            "success": True,
            "ui_loaded": True,
            "services_ready": True,
            "startup_time": 3.5,
            "error": None
        }
    
    def _execute_automation_workflow(self) -> Dict[str, Any]:
        """æ‰§è¡Œè‡ªåŠ¨åŒ–å·¥ä½œæµ"""
        return {
            "workflow_completed": True,
            "tasks_executed": 5,
            "execution_time": 12.3,
            "success_rate": 1.0
        }
    
    def _simulate_client_error(self) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿå®¢æˆ·ç«¯é”™è¯¯"""
        return {
            "error_handled": True,
            "recovery_successful": True,
            "recovery_time": 2.1
        }

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
'''
        
        client_test_path = self.modules["client_side"] / "test_client_e2e.py"
        with open(client_test_path, 'w', encoding='utf-8') as f:
            f.write(client_test_content)
        
        print(f"âœ… åˆ›å»ºå®¢æˆ·ç«¯æµ‹è¯•: {client_test_path}")
    
    def _create_server_side_tests(self):
        """åˆ›å»ºæœåŠ¡ç«¯æµ‹è¯•"""
        server_test_content = '''#!/usr/bin/env python3
"""
PowerAutomation æœåŠ¡ç«¯ç«¯åˆ°ç«¯æµ‹è¯•

æµ‹è¯•æœåŠ¡ç«¯åŠŸèƒ½çš„ç«¯åˆ°ç«¯æµç¨‹
"""

import pytest
import sys
from pathlib import Path

# æ·»åŠ æµ‹è¯•æ¡†æ¶è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))
from test_preconditions import PreconditionValidator

class TestServerSideE2E:
    """æœåŠ¡ç«¯ç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.validator = PreconditionValidator()
        cls.test_config = {
            "test_id": "SERVER_E2E_001",
            "test_name": "æœåŠ¡ç«¯ç«¯åˆ°ç«¯æµ‹è¯•",
            "preconditions": {
                "platform": {
                    "required_platforms": ["linux"],
                    "preferred_platforms": ["linux"],
                    "excluded_platforms": []
                },
                "resources": {
                    "min_memory_gb": 16,
                    "min_cpu_cores": 8,
                    "gpu_required": False
                },
                "capabilities": ["api_test", "data_test", "performance_test"],
                "environment": {
                    "database": "PostgreSQL 14+",
                    "cache": "Redis 7.0+",
                    "web_server": "Nginx 1.20+"
                },
                "dependencies": ["database_engine", "cache_system", "api_gateway"]
            }
        }
    
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®"""
        validation_result = self.validator.validate_preconditions(
            self.test_config["preconditions"]
        )
        
        if not validation_result["valid"]:
            pytest.skip(f"å‰ç½®æ¡ä»¶ä¸æ»¡è¶³: {validation_result['reason']}")
    
    def test_server_api_endpoints(self):
        """æµ‹è¯•æœåŠ¡ç«¯APIç«¯ç‚¹"""
        # æµ‹è¯•APIç«¯ç‚¹
        api_result = self._test_api_endpoints()
        
        assert api_result["all_endpoints_working"], "éƒ¨åˆ†APIç«¯ç‚¹ä¸å·¥ä½œ"
        assert api_result["response_time"] < 1.0, "APIå“åº”æ—¶é—´è¿‡é•¿"
    
    def test_server_data_processing(self):
        """æµ‹è¯•æœåŠ¡ç«¯æ•°æ®å¤„ç†"""
        # æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹
        data_result = self._test_data_processing()
        
        assert data_result["data_processed"], "æ•°æ®å¤„ç†å¤±è´¥"
        assert data_result["data_integrity"], "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥"
    
    def test_server_performance(self):
        """æµ‹è¯•æœåŠ¡ç«¯æ€§èƒ½"""
        # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
        perf_result = self._test_server_performance()
        
        assert perf_result["throughput"] > 1000, "æœåŠ¡å™¨ååé‡ä¸è¶³"
        assert perf_result["cpu_usage"] < 80, "CPUä½¿ç”¨ç‡è¿‡é«˜"
        assert perf_result["memory_usage"] < 80, "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
    
    def _test_api_endpoints(self) -> Dict[str, Any]:
        """æµ‹è¯•APIç«¯ç‚¹"""
        return {
            "all_endpoints_working": True,
            "response_time": 0.5,
            "endpoints_tested": 15,
            "success_rate": 1.0
        }
    
    def _test_data_processing(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®å¤„ç†"""
        return {
            "data_processed": True,
            "data_integrity": True,
            "processing_time": 5.2,
            "records_processed": 10000
        }
    
    def _test_server_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æœåŠ¡å™¨æ€§èƒ½"""
        return {
            "throughput": 1500,
            "cpu_usage": 65,
            "memory_usage": 70,
            "response_time": 0.3
        }

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
'''
        
        server_test_path = self.modules["server_side"] / "test_server_e2e.py"
        with open(server_test_path, 'w', encoding='utf-8') as f:
            f.write(server_test_content)
        
        print(f"âœ… åˆ›å»ºæœåŠ¡ç«¯æµ‹è¯•: {server_test_path}")
    
    def _create_integration_tests(self):
        """åˆ›å»ºé›†æˆæµ‹è¯•"""
        integration_test_content = '''#!/usr/bin/env python3
"""
PowerAutomation é›†æˆç«¯åˆ°ç«¯æµ‹è¯•

æµ‹è¯•å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯çš„é›†æˆæµç¨‹
"""

import pytest
import sys
from pathlib import Path

# æ·»åŠ æµ‹è¯•æ¡†æ¶è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))
from test_preconditions import PreconditionValidator

class TestIntegrationE2E:
    """é›†æˆç«¯åˆ°ç«¯æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.validator = PreconditionValidator()
        cls.test_config = {
            "test_id": "INTEGRATION_E2E_001",
            "test_name": "é›†æˆç«¯åˆ°ç«¯æµ‹è¯•",
            "preconditions": {
                "platform": {
                    "required_platforms": ["windows", "macos", "linux"],
                    "preferred_platforms": ["linux"],
                    "excluded_platforms": []
                },
                "resources": {
                    "min_memory_gb": 16,
                    "min_cpu_cores": 8,
                    "gpu_required": False
                },
                "capabilities": ["integration_test", "api_test", "ui_test"],
                "environment": {
                    "network": "stable",
                    "latency": "<100ms"
                },
                "dependencies": ["client_app", "server_api", "database"]
            }
        }
    
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®"""
        validation_result = self.validator.validate_preconditions(
            self.test_config["preconditions"]
        )
        
        if not validation_result["valid"]:
            pytest.skip(f"å‰ç½®æ¡ä»¶ä¸æ»¡è¶³: {validation_result['reason']}")
    
    def test_client_server_communication(self):
        """æµ‹è¯•å®¢æˆ·ç«¯æœåŠ¡ç«¯é€šä¿¡"""
        # æµ‹è¯•é€šä¿¡æµç¨‹
        comm_result = self._test_communication()
        
        assert comm_result["connection_established"], "è¿æ¥å»ºç«‹å¤±è´¥"
        assert comm_result["data_exchange_successful"], "æ•°æ®äº¤æ¢å¤±è´¥"
    
    def test_end_to_end_workflow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµ"""
        # æ‰§è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµ
        workflow_result = self._execute_e2e_workflow()
        
        assert workflow_result["workflow_completed"], "ç«¯åˆ°ç«¯å·¥ä½œæµæœªå®Œæˆ"
        assert workflow_result["all_components_working"], "éƒ¨åˆ†ç»„ä»¶ä¸å·¥ä½œ"
    
    def test_integration_error_handling(self):
        """æµ‹è¯•é›†æˆé”™è¯¯å¤„ç†"""
        # æµ‹è¯•é›†æˆé”™è¯¯å¤„ç†
        error_result = self._test_integration_errors()
        
        assert error_result["errors_handled"], "é›†æˆé”™è¯¯æœªè¢«å¤„ç†"
        assert error_result["system_recovered"], "ç³»ç»Ÿæœªæ¢å¤"
    
    def _test_communication(self) -> Dict[str, Any]:
        """æµ‹è¯•é€šä¿¡"""
        return {
            "connection_established": True,
            "data_exchange_successful": True,
            "latency": 50,
            "throughput": 1000
        }
    
    def _execute_e2e_workflow(self) -> Dict[str, Any]:
        """æ‰§è¡Œç«¯åˆ°ç«¯å·¥ä½œæµ"""
        return {
            "workflow_completed": True,
            "all_components_working": True,
            "execution_time": 30.5,
            "success_rate": 0.95
        }
    
    def _test_integration_errors(self) -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆé”™è¯¯"""
        return {
            "errors_handled": True,
            "system_recovered": True,
            "recovery_time": 5.0
        }

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
'''
        
        integration_test_path = self.modules["integration"] / "test_integration_e2e.py"
        with open(integration_test_path, 'w', encoding='utf-8') as f:
            f.write(integration_test_content)
        
        print(f"âœ… åˆ›å»ºé›†æˆæµ‹è¯•: {integration_test_path}")
    
    def _create_e2e_config(self):
        """åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•é…ç½®"""
        e2e_config = {
            "end_to_end_tests": {
                "test_levels": {
                    "client_side": {
                        "description": "å®¢æˆ·ç«¯ç«¯åˆ°ç«¯æµ‹è¯•",
                        "platforms": ["windows", "macos"],
                        "test_types": ["ui_test", "automation_test"],
                        "timeout": 300
                    },
                    "server_side": {
                        "description": "æœåŠ¡ç«¯ç«¯åˆ°ç«¯æµ‹è¯•",
                        "platforms": ["linux"],
                        "test_types": ["api_test", "data_test", "performance_test"],
                        "timeout": 600
                    },
                    "integration": {
                        "description": "é›†æˆç«¯åˆ°ç«¯æµ‹è¯•",
                        "platforms": ["windows", "macos", "linux"],
                        "test_types": ["integration_test", "api_test", "ui_test"],
                        "timeout": 900
                    },
                    "fallback_automation": {
                        "description": "å…œåº•è‡ªåŠ¨åŒ–ç«¯åˆ°ç«¯æµ‹è¯•",
                        "platforms": ["windows", "macos", "linux"],
                        "test_types": ["fallback_test", "automation_test", "ui_test"],
                        "timeout": 600
                    }
                },
                "execution_order": [
                    "server_side",
                    "client_side", 
                    "integration",
                    "fallback_automation"
                ],
                "parallel_execution": {
                    "enabled": True,
                    "max_workers": 4
                },
                "reporting": {
                    "format": ["html", "json", "xml"],
                    "output_dir": "e2e_reports"
                }
            }
        }
        
        config_path = self.e2e_dir / "configs" / "e2e_config.yaml"
        config_path.parent.mkdir(exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(e2e_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… åˆ›å»ºç«¯åˆ°ç«¯é…ç½®: {config_path}")
    
    def _create_e2e_runner(self):
        """åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨"""
        runner_content = '''#!/usr/bin/env python3
"""
PowerAutomation ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨

ç»Ÿä¸€ç®¡ç†å’Œæ‰§è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•
"""

import os
import sys
import yaml
import pytest
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class E2ETestRunner:
    """ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.e2e_dir = Path(__file__).parent
        self.config_path = self.e2e_dir / "configs" / "e2e_config.yaml"
        self.config = self._load_config()
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        self.report_dir = self.e2e_dir / "e2e_reports"
        self.report_dir.mkdir(exist_ok=True)
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        if self.config_path.exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}
    
    def run_all_e2e_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ‰§è¡ŒPowerAutomationç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶...")
        
        test_config = self.config.get("end_to_end_tests", {})
        execution_order = test_config.get("execution_order", [])
        
        results = {}
        overall_success = True
        
        for test_level in execution_order:
            print(f"\\nğŸ“‹ æ‰§è¡Œ {test_level} æµ‹è¯•...")
            
            result = self._run_test_level(test_level)
            results[test_level] = result
            
            if not result["success"]:
                overall_success = False
                print(f"âŒ {test_level} æµ‹è¯•å¤±è´¥")
            else:
                print(f"âœ… {test_level} æµ‹è¯•æˆåŠŸ")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report(results)
        
        if overall_success:
            print("\\nğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print("\\nâš ï¸ éƒ¨åˆ†ç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
        
        return overall_success
    
    def _run_test_level(self, test_level: str) -> Dict[str, Any]:
        """è¿è¡Œç‰¹å®šçº§åˆ«çš„æµ‹è¯•"""
        test_dir = self.e2e_dir / test_level
        
        if not test_dir.exists():
            return {
                "success": False,
                "error": f"æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}",
                "execution_time": 0
            }
        
        # æ„å»ºpytestå‚æ•°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.report_dir / f"{test_level}_report_{timestamp}.html"
        
        pytest_args = [
            str(test_dir),
            "-v",
            "--tb=short",
            "--capture=no",
            f"--html={report_file}",
            "--self-contained-html"
        ]
        
        try:
            start_time = datetime.now()
            result = pytest.main(pytest_args)
            end_time = datetime.now()
            
            execution_time = (end_time - start_time).total_seconds()
            
            return {
                "success": result == 0,
                "exit_code": result,
                "execution_time": execution_time,
                "report_file": str(report_file),
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "execution_time": 0
            }
    
    def run_specific_test(self, test_level: str, test_name: str = None) -> bool:
        """è¿è¡Œç‰¹å®šçš„æµ‹è¯•"""
        test_dir = self.e2e_dir / test_level
        
        if test_name:
            test_file = test_dir / f"test_{test_name}.py"
            if not test_file.exists():
                print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
                return False
            target = str(test_file)
        else:
            target = str(test_dir)
        
        pytest_args = [target, "-v"]
        result = pytest.main(pytest_args)
        
        return result == 0
    
    def _generate_comprehensive_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_data = {
            "execution_timestamp": timestamp,
            "overall_success": all(r.get("success", False) for r in results.values()),
            "test_results": results,
            "summary": {
                "total_test_levels": len(results),
                "successful_levels": sum(1 for r in results.values() if r.get("success", False)),
                "failed_levels": sum(1 for r in results.values() if not r.get("success", False)),
                "total_execution_time": sum(r.get("execution_time", 0) for r in results.values())
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_path = self.report_dir / f"e2e_comprehensive_report_{timestamp}.json"
        import json
        with open(json_report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\\nğŸ“Š ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {json_report_path}")

if __name__ == "__main__":
    runner = E2ETestRunner()
    
    if len(sys.argv) > 1:
        # è¿è¡Œç‰¹å®šæµ‹è¯•
        test_level = sys.argv[1]
        test_name = sys.argv[2] if len(sys.argv) > 2 else None
        runner.run_specific_test(test_level, test_name)
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        runner.run_all_e2e_tests()
'''
        
        runner_path = self.e2e_dir / "e2e_test_runner.py"
        with open(runner_path, 'w', encoding='utf-8') as f:
            f.write(runner_content)
        
        print(f"âœ… åˆ›å»ºç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨: {runner_path}")

if __name__ == "__main__":
    manager = EndToEndTestManager()
    manager.initialize_e2e_structure()

