#!/usr/bin/env python3
"""
PowerAutomationä¸€é”®å½•åˆ¶å·¥ä½œæµç³»ç»Ÿå®Œæ•´æ€§æµ‹è¯•å’Œä¼˜åŒ–

å¯¹æ•´ä¸ªä¸€é”®å½•åˆ¶å·¥ä½œæµç³»ç»Ÿè¿›è¡Œå…¨é¢æµ‹è¯•ã€æ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½éªŒè¯
ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ã€å¯é æ€§å’Œæ˜“ç”¨æ€§
"""

import os
import sys
import json
import time
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

# å¯¼å…¥æ‰€æœ‰ç»„ä»¶
sys.path.append(str(Path(__file__).parent))
from workflow_recorder_integration import WorkflowRecorder, WorkflowRecordingConfig
from kilo_code_recorder import KiloCodeRecorder, StruggleModeType, InterventionType
from n8n_workflow_converter import N8nWorkflowConverter
from visual_workflow_integrator import VisualWorkflowIntegrator, VisualRecordingConfig

@dataclass
class SystemTestConfig:
    """ç³»ç»Ÿæµ‹è¯•é…ç½®"""
    test_all_scenarios: bool = True
    test_performance: bool = True
    test_error_handling: bool = True
    test_integration: bool = True
    generate_comprehensive_report: bool = True
    cleanup_test_data: bool = False

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_name: str
    test_type: str
    success: bool
    duration: float
    error_message: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None

class SystemTester:
    """ç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self, test_framework_dir: str = None):
        # è®¾ç½®æµ‹è¯•ç›®å½•
        if test_framework_dir:
            self.framework_dir = Path(test_framework_dir)
        else:
            self.framework_dir = Path(__file__).parent
        
        # åˆ›å»ºæµ‹è¯•ä¸“ç”¨ç›®å½•
        self.test_dir = self.framework_dir / "system_tests"
        self.test_results_dir = self.test_dir / "results"
        self.test_data_dir = self.test_dir / "test_data"
        self.performance_dir = self.test_dir / "performance"
        
        for directory in [self.test_dir, self.test_results_dir, self.test_data_dir, self.performance_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.workflow_recorder = WorkflowRecorder(str(self.framework_dir))
        self.kilo_code_recorder = KiloCodeRecorder(str(self.framework_dir))
        self.n8n_converter = N8nWorkflowConverter(str(self.test_dir / "n8n_test"))
        self.visual_integrator = VisualWorkflowIntegrator(str(self.framework_dir))
        
        # æµ‹è¯•ç»“æœ
        self.test_results: List[TestResult] = []
        self.system_metrics = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "total_duration": 0.0,
            "average_test_duration": 0.0,
            "performance_metrics": {},
            "error_summary": {}
        }
    
    def run_comprehensive_tests(self, config: SystemTestConfig = None) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„ç³»ç»Ÿæµ‹è¯•"""
        
        config = config or SystemTestConfig()
        start_time = time.time()
        
        print("ğŸ§ª å¼€å§‹PowerAutomationä¸€é”®å½•åˆ¶ç³»ç»Ÿå…¨é¢æµ‹è¯•")
        print("=" * 60)
        
        # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
        print("\nğŸ“‹ 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        self._test_basic_functionality()
        
        # 2. Kilo Codeåœºæ™¯æµ‹è¯•
        if config.test_all_scenarios:
            print("\nğŸ¯ 2. Kilo Codeåœºæ™¯æµ‹è¯•")
            self._test_kilo_code_scenarios()
        
        # 3. n8nå·¥ä½œæµè½¬æ¢æµ‹è¯•
        print("\nğŸ”„ 3. n8nå·¥ä½œæµè½¬æ¢æµ‹è¯•")
        self._test_n8n_conversion()
        
        # 4. è§†è§‰é›†æˆæµ‹è¯•
        if config.test_integration:
            print("\nğŸ‘ï¸ 4. è§†è§‰é›†æˆæµ‹è¯•")
            self._test_visual_integration()
        
        # 5. æ€§èƒ½æµ‹è¯•
        if config.test_performance:
            print("\nâš¡ 5. æ€§èƒ½æµ‹è¯•")
            self._test_performance()
        
        # 6. é”™è¯¯å¤„ç†æµ‹è¯•
        if config.test_error_handling:
            print("\nğŸš¨ 6. é”™è¯¯å¤„ç†æµ‹è¯•")
            self._test_error_handling()
        
        # 7. é›†æˆæµ‹è¯•
        if config.test_integration:
            print("\nğŸ”— 7. ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
            self._test_end_to_end_integration()
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_duration = time.time() - start_time
        self._calculate_system_metrics(total_duration)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        test_report = self._generate_test_report(config)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self._save_test_results(test_report)
        
        # æ¸…ç†æµ‹è¯•æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.cleanup_test_data:
            self._cleanup_test_data()
        
        print(f"\nğŸ‰ ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
        print(f"   æ€»æµ‹è¯•æ•°: {self.system_metrics['total_tests']}")
        print(f"   é€šè¿‡: {self.system_metrics['passed_tests']}")
        print(f"   å¤±è´¥: {self.system_metrics['failed_tests']}")
        print(f"   æ€»è€—æ—¶: {self.system_metrics['total_duration']:.2f}s")
        
        return test_report
    
    def _test_basic_functionality(self):
        """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
        
        # æµ‹è¯•å·¥ä½œæµå½•åˆ¶å™¨
        self._run_test("workflow_recorder_basic", "basic", self._test_workflow_recorder_basic)
        
        # æµ‹è¯•Kilo Codeå½•åˆ¶å™¨
        self._run_test("kilo_code_recorder_basic", "basic", self._test_kilo_code_recorder_basic)
        
        # æµ‹è¯•n8nè½¬æ¢å™¨
        self._run_test("n8n_converter_basic", "basic", self._test_n8n_converter_basic)
    
    def _test_workflow_recorder_basic(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµå½•åˆ¶å™¨åŸºç¡€åŠŸèƒ½"""
        
        config = WorkflowRecordingConfig(
            recording_mode="general_test",
            target_version="enterprise",
            enable_visual_verification=False,
            enable_screenshot=False
        )
        
        # å¼€å§‹å½•åˆ¶
        recording_id = self.workflow_recorder.start_recording(
            recording_name="åŸºç¡€åŠŸèƒ½æµ‹è¯•",
            config=config,
            description="æµ‹è¯•å·¥ä½œæµå½•åˆ¶å™¨åŸºç¡€åŠŸèƒ½"
        )
        
        # å½•åˆ¶ä¸€äº›åŠ¨ä½œ
        self.workflow_recorder.record_user_action("test_action", {"test": "data"})
        self.workflow_recorder.record_ui_interaction(
            "click", 
            {"selector": ".test-button"},
            {"action": "test_click"}
        )
        
        # åœæ­¢å½•åˆ¶
        result = self.workflow_recorder.stop_recording()
        
        # éªŒè¯ç»“æœ
        assert result is not None, "å½•åˆ¶ç»“æœä¸èƒ½ä¸ºç©º"
        assert result["recording_id"] == recording_id, "å½•åˆ¶IDä¸åŒ¹é…"
        assert len(result["actions"]) >= 2, "åŠ¨ä½œæ•°é‡ä¸è¶³"
        
        return {
            "recording_id": recording_id,
            "actions_count": len(result["actions"]),
            "duration": result.get("statistics", {}).get("recording_duration", 0)
        }
    
    def _test_kilo_code_recorder_basic(self) -> Dict[str, Any]:
        """æµ‹è¯•Kilo Codeå½•åˆ¶å™¨åŸºç¡€åŠŸèƒ½"""
        
        # å¼€å§‹åœºæ™¯å½•åˆ¶
        recording_id = self.kilo_code_recorder.start_scenario_recording("enterprise_critical_modes")
        
        # å½•åˆ¶æŒ£æ‰æ¨¡å¼
        self.kilo_code_recorder.record_struggle_mode_detection(
            struggle_mode=StruggleModeType.SYNTAX_ERROR,
            detection_data={"error_type": "test_error"},
            confidence_score=0.95,
            response_time=1.0
        )
        
        # å½•åˆ¶ä»‹å…¥
        self.kilo_code_recorder.record_intervention_trigger(
            intervention_type=InterventionType.CODE_SUGGESTION,
            intervention_data={"suggestion": "test_suggestion"},
            success_rate=0.90,
            response_time=0.5
        )
        
        # åœæ­¢å½•åˆ¶
        result = self.kilo_code_recorder.stop_scenario_recording()
        
        # éªŒè¯ç»“æœ
        assert result is not None, "Kilo Codeå½•åˆ¶ç»“æœä¸èƒ½ä¸ºç©º"
        assert result["recording_info"]["recording_id"] == recording_id, "å½•åˆ¶IDä¸åŒ¹é…"
        assert result["performance_analysis"]["average_response_time"] > 0, "å“åº”æ—¶é—´æ— æ•ˆ"
        
        return {
            "recording_id": recording_id,
            "kilo_events_count": len(result.get("raw_recording_data", {}).get("kilo_code_events", [])),
            "quality_score": result["quality_assessment"]["overall_quality_score"]
        }
    
    def _test_n8n_converter_basic(self) -> Dict[str, Any]:
        """æµ‹è¯•n8nè½¬æ¢å™¨åŸºç¡€åŠŸèƒ½"""
        
        # åˆ›å»ºæµ‹è¯•å½•åˆ¶æ•°æ®
        test_recording = {
            "recording_id": "test_n8n_001",
            "recording_name": "n8nè½¬æ¢æµ‹è¯•",
            "recording_mode": "kilo_code_detection",
            "target_version": "enterprise",
            "kilo_code_events": [
                {
                    "event_id": "test_event_001",
                    "detection_type": "struggle_mode_1",
                    "detection_data": {"test": "data"},
                    "confidence_score": 0.95,
                    "response_time": 1.0
                }
            ],
            "actions": [
                {
                    "id": "test_action_001",
                    "action_name": "click",
                    "element_selector": ".test"
                }
            ],
            "statistics": {
                "total_kilo_code_events": 1,
                "average_kilo_code_response_time": 1.0
            }
        }
        
        # è½¬æ¢ä¸ºn8nå·¥ä½œæµ
        workflow = self.n8n_converter.convert_recording_to_n8n(test_recording)
        
        # ä¿å­˜å·¥ä½œæµ
        workflow_path = self.n8n_converter.save_workflow(workflow)
        
        # éªŒè¯ç»“æœ
        assert workflow is not None, "n8nå·¥ä½œæµä¸èƒ½ä¸ºç©º"
        assert len(workflow.nodes) > 0, "å·¥ä½œæµèŠ‚ç‚¹æ•°é‡ä¸èƒ½ä¸º0"
        assert workflow_path is not None, "å·¥ä½œæµä¿å­˜è·¯å¾„ä¸èƒ½ä¸ºç©º"
        
        return {
            "workflow_name": workflow.name,
            "nodes_count": len(workflow.nodes),
            "connections_count": len(workflow.connections),
            "workflow_path": workflow_path
        }
    
    def _test_kilo_code_scenarios(self):
        """æµ‹è¯•æ‰€æœ‰Kilo Codeåœºæ™¯"""
        
        scenarios = self.kilo_code_recorder.list_scenarios()
        
        for scenario in scenarios:
            scenario_id = scenario["scenario_id"]
            self._run_test(
                f"kilo_scenario_{scenario_id}", 
                "scenario", 
                lambda sid=scenario_id: self._test_single_kilo_scenario(sid)
            )
    
    def _test_single_kilo_scenario(self, scenario_id: str) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªKilo Codeåœºæ™¯"""
        
        # å¼€å§‹åœºæ™¯å½•åˆ¶
        recording_id = self.kilo_code_recorder.start_scenario_recording(scenario_id)
        
        # æ¨¡æ‹Ÿä¸€äº›æ£€æµ‹äº‹ä»¶
        self.kilo_code_recorder.record_struggle_mode_detection(
            struggle_mode=StruggleModeType.SYNTAX_ERROR,
            detection_data={"scenario": scenario_id},
            confidence_score=0.90,
            response_time=1.5
        )
        
        # åœæ­¢å½•åˆ¶
        result = self.kilo_code_recorder.stop_scenario_recording()
        
        return {
            "scenario_id": scenario_id,
            "recording_id": recording_id,
            "quality_score": result["quality_assessment"]["overall_quality_score"]
        }
    
    def _test_n8n_conversion(self):
        """æµ‹è¯•n8nå·¥ä½œæµè½¬æ¢"""
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„è½¬æ¢
        self._run_test("n8n_kilo_code_conversion", "conversion", self._test_n8n_kilo_code_conversion)
        self._run_test("n8n_general_conversion", "conversion", self._test_n8n_general_conversion)
    
    def _test_n8n_kilo_code_conversion(self) -> Dict[str, Any]:
        """æµ‹è¯•Kilo Codeä¸“ç”¨n8nè½¬æ¢"""
        
        # åˆ›å»ºå¤æ‚çš„Kilo Codeå½•åˆ¶æ•°æ®
        complex_recording = {
            "recording_id": "complex_kilo_001",
            "recording_name": "å¤æ‚Kilo Codeæµ‹è¯•",
            "recording_mode": "kilo_code_detection",
            "target_version": "enterprise",
            "kilo_code_events": [
                {
                    "event_id": "event_001",
                    "detection_type": "struggle_mode_1",
                    "detection_data": {"complexity": "high"},
                    "confidence_score": 0.95,
                    "response_time": 1.2
                },
                {
                    "event_id": "event_002",
                    "detection_type": "intervention_trigger",
                    "detection_data": {"type": "suggestion"},
                    "confidence_score": 0.88,
                    "response_time": 0.8
                }
            ],
            "statistics": {
                "total_kilo_code_events": 2,
                "average_kilo_code_response_time": 1.0
            }
        }
        
        # è½¬æ¢å¹¶éªŒè¯
        workflow = self.n8n_converter.convert_recording_to_n8n(complex_recording, "kilo_code_detection")
        workflow_path = self.n8n_converter.save_workflow(workflow)
        
        return {
            "nodes_count": len(workflow.nodes),
            "has_kilo_nodes": any("KiloCode" in node.name for node in workflow.nodes),
            "workflow_path": workflow_path
        }
    
    def _test_n8n_general_conversion(self) -> Dict[str, Any]:
        """æµ‹è¯•é€šç”¨n8nè½¬æ¢"""
        
        general_recording = {
            "recording_id": "general_001",
            "recording_name": "é€šç”¨æµ‹è¯•",
            "recording_mode": "general_test",
            "target_version": "personal_pro",
            "actions": [
                {"id": "action_001", "action_name": "click"},
                {"id": "action_002", "action_name": "input"}
            ],
            "statistics": {"total_actions": 2}
        }
        
        workflow = self.n8n_converter.convert_recording_to_n8n(general_recording, "general_test")
        workflow_path = self.n8n_converter.save_workflow(workflow)
        
        return {
            "nodes_count": len(workflow.nodes),
            "workflow_path": workflow_path
        }
    
    def _test_visual_integration(self):
        """æµ‹è¯•è§†è§‰é›†æˆåŠŸèƒ½"""
        
        # æ³¨æ„ï¼šç”±äºå†…å­˜é™åˆ¶ï¼Œè¿™é‡Œåªæµ‹è¯•é…ç½®å’Œåˆå§‹åŒ–
        self._run_test("visual_integration_config", "integration", self._test_visual_integration_config)
    
    def _test_visual_integration_config(self) -> Dict[str, Any]:
        """æµ‹è¯•è§†è§‰é›†æˆé…ç½®"""
        
        # æµ‹è¯•é…ç½®åˆ›å»º
        recording_config = WorkflowRecordingConfig(
            recording_mode="kilo_code_detection",
            target_version="enterprise",
            enable_visual_verification=False  # ç¦ç”¨ä»¥é¿å…å†…å­˜é—®é¢˜
        )
        
        visual_config = VisualRecordingConfig(
            enable_visual_verification=False,  # ç¦ç”¨ä»¥é¿å…å†…å­˜é—®é¢˜
            enable_screenshot_comparison=False,
            capture_on_kilo_events=False
        )
        
        # éªŒè¯é…ç½®
        assert recording_config.recording_mode == "kilo_code_detection"
        assert visual_config.enable_visual_verification == False
        
        return {
            "recording_config_valid": True,
            "visual_config_valid": True,
            "integration_ready": True
        }
    
    def _test_performance(self):
        """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½"""
        
        self._run_test("performance_recording_speed", "performance", self._test_recording_speed)
        self._run_test("performance_conversion_speed", "performance", self._test_conversion_speed)
        self._run_test("performance_memory_usage", "performance", self._test_memory_usage)
    
    def _test_recording_speed(self) -> Dict[str, Any]:
        """æµ‹è¯•å½•åˆ¶é€Ÿåº¦"""
        
        start_time = time.time()
        
        # å¿«é€Ÿå½•åˆ¶æµ‹è¯•
        config = WorkflowRecordingConfig(
            recording_mode="performance_test",
            enable_visual_verification=False,
            enable_screenshot=False
        )
        
        recording_id = self.workflow_recorder.start_recording("æ€§èƒ½æµ‹è¯•", config)
        
        # å½•åˆ¶å¤šä¸ªåŠ¨ä½œ
        for i in range(10):
            self.workflow_recorder.record_user_action(f"action_{i}", {"index": i})
        
        result = self.workflow_recorder.stop_recording()
        
        duration = time.time() - start_time
        
        return {
            "total_duration": duration,
            "actions_count": len(result["actions"]),
            "actions_per_second": len(result["actions"]) / duration if duration > 0 else 0
        }
    
    def _test_conversion_speed(self) -> Dict[str, Any]:
        """æµ‹è¯•è½¬æ¢é€Ÿåº¦"""
        
        # åˆ›å»ºå¤§å‹å½•åˆ¶æ•°æ®
        large_recording = {
            "recording_id": "large_test_001",
            "recording_name": "å¤§å‹è½¬æ¢æµ‹è¯•",
            "recording_mode": "kilo_code_detection",
            "target_version": "enterprise",
            "kilo_code_events": [
                {
                    "event_id": f"event_{i:03d}",
                    "detection_type": "struggle_mode_1",
                    "detection_data": {"index": i},
                    "confidence_score": 0.90,
                    "response_time": 1.0
                }
                for i in range(20)
            ],
            "statistics": {"total_kilo_code_events": 20}
        }
        
        start_time = time.time()
        workflow = self.n8n_converter.convert_recording_to_n8n(large_recording)
        conversion_duration = time.time() - start_time
        
        start_time = time.time()
        workflow_path = self.n8n_converter.save_workflow(workflow)
        save_duration = time.time() - start_time
        
        return {
            "conversion_duration": conversion_duration,
            "save_duration": save_duration,
            "total_duration": conversion_duration + save_duration,
            "events_converted": 20,
            "nodes_generated": len(workflow.nodes)
        }
    
    def _test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå†…å­˜å¯†é›†æ“ä½œ
        large_data = []
        for i in range(100):
            large_data.append({
                "recording_id": f"memory_test_{i}",
                "data": ["test"] * 1000
            })
        
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ¸…ç†
        del large_data
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        return {
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": peak_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": peak_memory - initial_memory,
            "memory_recovered_mb": peak_memory - final_memory
        }
    
    def _test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        
        self._run_test("error_invalid_config", "error", self._test_invalid_config_error)
        self._run_test("error_invalid_data", "error", self._test_invalid_data_error)
    
    def _test_invalid_config_error(self) -> Dict[str, Any]:
        """æµ‹è¯•æ— æ•ˆé…ç½®é”™è¯¯å¤„ç†"""
        
        try:
            # å°è¯•ä½¿ç”¨æ— æ•ˆåœºæ™¯ID
            self.kilo_code_recorder.start_scenario_recording("invalid_scenario_id")
            return {"error_handled": False, "error_type": "none"}
        except ValueError as e:
            return {"error_handled": True, "error_type": "ValueError", "error_message": str(e)}
        except Exception as e:
            return {"error_handled": True, "error_type": type(e).__name__, "error_message": str(e)}
    
    def _test_invalid_data_error(self) -> Dict[str, Any]:
        """æµ‹è¯•æ— æ•ˆæ•°æ®é”™è¯¯å¤„ç†"""
        
        try:
            # å°è¯•è½¬æ¢æ— æ•ˆå½•åˆ¶æ•°æ®
            invalid_data = {"invalid": "data"}
            self.n8n_converter.convert_recording_to_n8n(invalid_data)
            return {"error_handled": False, "error_type": "none"}
        except Exception as e:
            return {"error_handled": True, "error_type": type(e).__name__, "error_message": str(e)}
    
    def _test_end_to_end_integration(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯é›†æˆ"""
        
        self._run_test("e2e_complete_workflow", "integration", self._test_complete_workflow)
    
    def _test_complete_workflow(self) -> Dict[str, Any]:
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
        
        # 1. å¼€å§‹Kilo Codeå½•åˆ¶
        recording_id = self.kilo_code_recorder.start_scenario_recording("enterprise_critical_modes")
        
        # 2. å½•åˆ¶äº‹ä»¶
        self.kilo_code_recorder.record_struggle_mode_detection(
            struggle_mode=StruggleModeType.SYNTAX_ERROR,
            detection_data={"integration_test": True},
            confidence_score=0.95,
            response_time=1.0
        )
        
        # 3. åœæ­¢å½•åˆ¶
        kilo_result = self.kilo_code_recorder.stop_scenario_recording()
        
        # 4. è½¬æ¢ä¸ºn8nå·¥ä½œæµ
        workflow = self.n8n_converter.convert_recording_to_n8n(
            kilo_result["raw_recording_data"], "kilo_code_detection"
        )
        
        # 5. ä¿å­˜å·¥ä½œæµ
        workflow_path = self.n8n_converter.save_workflow(workflow)
        
        return {
            "kilo_recording_success": kilo_result is not None,
            "n8n_conversion_success": workflow is not None,
            "workflow_save_success": workflow_path is not None,
            "end_to_end_success": True,
            "workflow_path": workflow_path
        }
    
    def _run_test(self, test_name: str, test_type: str, test_func) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        
        print(f"   ğŸ§ª {test_name}...", end=" ")
        
        start_time = time.time()
        success = False
        error_message = None
        metrics = None
        
        try:
            metrics = test_func()
            success = True
            print("âœ…")
        except Exception as e:
            error_message = str(e)
            print(f"âŒ ({error_message})")
        
        duration = time.time() - start_time
        
        result = TestResult(
            test_name=test_name,
            test_type=test_type,
            success=success,
            duration=duration,
            error_message=error_message,
            metrics=metrics
        )
        
        self.test_results.append(result)
        return result
    
    def _calculate_system_metrics(self, total_duration: float):
        """è®¡ç®—ç³»ç»ŸæŒ‡æ ‡"""
        
        self.system_metrics["total_tests"] = len(self.test_results)
        self.system_metrics["passed_tests"] = sum(1 for r in self.test_results if r.success)
        self.system_metrics["failed_tests"] = sum(1 for r in self.test_results if not r.success)
        self.system_metrics["total_duration"] = total_duration
        
        if self.test_results:
            self.system_metrics["average_test_duration"] = sum(r.duration for r in self.test_results) / len(self.test_results)
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        test_types = {}
        for result in self.test_results:
            test_type = result.test_type
            if test_type not in test_types:
                test_types[test_type] = {"total": 0, "passed": 0, "failed": 0}
            
            test_types[test_type]["total"] += 1
            if result.success:
                test_types[test_type]["passed"] += 1
            else:
                test_types[test_type]["failed"] += 1
        
        self.system_metrics["test_types"] = test_types
        
        # æ€§èƒ½æŒ‡æ ‡
        performance_tests = [r for r in self.test_results if r.test_type == "performance" and r.metrics]
        if performance_tests:
            self.system_metrics["performance_metrics"] = {
                "recording_speed": next((r.metrics.get("actions_per_second", 0) for r in performance_tests if "recording_speed" in r.test_name), 0),
                "conversion_speed": next((r.metrics.get("events_converted", 0) / r.metrics.get("conversion_duration", 1) for r in performance_tests if "conversion_speed" in r.test_name), 0),
                "memory_usage": next((r.metrics for r in performance_tests if "memory_usage" in r.test_name), {})
            }
        
        # é”™è¯¯æ‘˜è¦
        failed_tests = [r for r in self.test_results if not r.success]
        error_types = {}
        for test in failed_tests:
            error_type = test.error_message.split(":")[0] if test.error_message else "Unknown"
            if error_type not in error_types:
                error_types[error_type] = 0
            error_types[error_type] += 1
        
        self.system_metrics["error_summary"] = error_types
    
    def _generate_test_report(self, config: SystemTestConfig) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        report = {
            "test_summary": {
                "test_config": asdict(config),
                "execution_time": datetime.now().isoformat(),
                "system_metrics": self.system_metrics,
                "success_rate": self.system_metrics["passed_tests"] / max(self.system_metrics["total_tests"], 1)
            },
            "detailed_results": [asdict(result) for result in self.test_results],
            "component_status": {
                "workflow_recorder": self._assess_component_status("workflow_recorder"),
                "kilo_code_recorder": self._assess_component_status("kilo_code"),
                "n8n_converter": self._assess_component_status("n8n"),
                "visual_integrator": self._assess_component_status("visual")
            },
            "recommendations": self._generate_recommendations(),
            "system_health": self._assess_system_health()
        }
        
        return report
    
    def _assess_component_status(self, component_prefix: str) -> Dict[str, Any]:
        """è¯„ä¼°ç»„ä»¶çŠ¶æ€"""
        
        component_tests = [r for r in self.test_results if component_prefix in r.test_name]
        
        if not component_tests:
            return {"status": "not_tested", "success_rate": 0.0, "issues": []}
        
        passed = sum(1 for r in component_tests if r.success)
        total = len(component_tests)
        success_rate = passed / total
        
        status = "healthy" if success_rate >= 0.8 else "warning" if success_rate >= 0.5 else "critical"
        
        issues = [r.error_message for r in component_tests if not r.success and r.error_message]
        
        return {
            "status": status,
            "success_rate": success_rate,
            "tests_passed": passed,
            "tests_total": total,
            "issues": issues
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        success_rate = self.system_metrics["passed_tests"] / max(self.system_metrics["total_tests"], 1)
        
        if success_rate < 0.8:
            recommendations.append("ç³»ç»Ÿæ•´ä½“æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        # åŸºäºæ€§èƒ½çš„å»ºè®®
        perf_metrics = self.system_metrics.get("performance_metrics", {})
        if perf_metrics.get("recording_speed", 0) < 5:
            recommendations.append("å½•åˆ¶é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–å½•åˆ¶æ€§èƒ½")
        
        memory_usage = perf_metrics.get("memory_usage", {})
        if memory_usage.get("memory_increase_mb", 0) > 100:
            recommendations.append("å†…å­˜ä½¿ç”¨é‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†")
        
        # åŸºäºé”™è¯¯ç±»å‹çš„å»ºè®®
        error_summary = self.system_metrics.get("error_summary", {})
        if "ValueError" in error_summary:
            recommendations.append("å­˜åœ¨å‚æ•°éªŒè¯é—®é¢˜ï¼Œå»ºè®®åŠ å¼ºè¾“å…¥éªŒè¯")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations
    
    def _assess_system_health(self) -> str:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶å†µ"""
        
        success_rate = self.system_metrics["passed_tests"] / max(self.system_metrics["total_tests"], 1)
        
        if success_rate >= 0.95:
            return "excellent"
        elif success_rate >= 0.85:
            return "good"
        elif success_rate >= 0.70:
            return "fair"
        elif success_rate >= 0.50:
            return "poor"
        else:
            return "critical"
    
    def _save_test_results(self, test_report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = self.test_results_dir / f"system_test_report_{timestamp}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ç®€åŒ–æ‘˜è¦
        summary = {
            "timestamp": timestamp,
            "total_tests": self.system_metrics["total_tests"],
            "passed_tests": self.system_metrics["passed_tests"],
            "failed_tests": self.system_metrics["failed_tests"],
            "success_rate": test_report["test_summary"]["success_rate"],
            "system_health": test_report["system_health"],
            "duration": self.system_metrics["total_duration"]
        }
        
        summary_path = self.test_results_dir / f"system_test_summary_{timestamp}.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"   æ‘˜è¦: {summary_path}")
    
    def _cleanup_test_data(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
        
        # æ¸…ç†æµ‹è¯•ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆä¿ç•™æŠ¥å‘Šï¼‰
        cleanup_dirs = [
            self.test_data_dir,
            self.framework_dir / "workflow_recordings" / "flows",
            self.framework_dir / "workflow_recordings" / "n8n_workflows"
        ]
        
        for cleanup_dir in cleanup_dirs:
            if cleanup_dir.exists():
                for file in cleanup_dir.glob("*test*"):
                    try:
                        if file.is_file():
                            file.unlink()
                        print(f"   æ¸…ç†: {file.name}")
                    except Exception as e:
                        print(f"   æ¸…ç†å¤±è´¥: {file.name} - {e}")

if __name__ == "__main__":
    # è¿è¡Œç³»ç»Ÿæµ‹è¯•
    tester = SystemTester()
    
    # é…ç½®æµ‹è¯•
    test_config = SystemTestConfig(
        test_all_scenarios=True,
        test_performance=True,
        test_error_handling=True,
        test_integration=True,
        generate_comprehensive_report=True,
        cleanup_test_data=False  # ä¿ç•™æµ‹è¯•æ•°æ®ä»¥ä¾›æ£€æŸ¥
    )
    
    # æ‰§è¡Œæµ‹è¯•
    test_report = tester.run_comprehensive_tests(test_config)
    
    print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š:")
    print(f"   ç³»ç»Ÿå¥åº·çŠ¶å†µ: {test_report['system_health'].upper()}")
    print(f"   æˆåŠŸç‡: {test_report['test_summary']['success_rate']:.1%}")
    print(f"   ç»„ä»¶çŠ¶æ€:")
    for component, status in test_report['component_status'].items():
        print(f"     {component}: {status['status'].upper()} ({status['success_rate']:.1%})")
    
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    for recommendation in test_report['recommendations']:
        print(f"   â€¢ {recommendation}")
    
    print(f"\nğŸ‰ PowerAutomationä¸€é”®å½•åˆ¶å·¥ä½œæµç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")

